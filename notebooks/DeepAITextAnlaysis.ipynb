{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepAI API Connection\n",
    "- sentimental analysis\n",
    "- text summarization\n",
    "- text tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '70a48b7c-b698-40e9-82f9-5d52130fd76a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '66e96a3f-aede-482f-8464-5a84c05969fb', 'output': ['Positive']}\n"
     ]
    }
   ],
   "source": [
    "r = requests.post(\n",
    "    \"https://api.deepai.org/api/sentiment-analysis\",\n",
    "    data={\n",
    "        'text': 'Benin is a great country',\n",
    "    },\n",
    "    headers={'api-key': api_key}\n",
    ")\n",
    "print(r.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "\n",
    "I am giving my interim report for the model, and it is giving me a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example posting a local text file:\n",
    "\n",
    "r = requests.post(\n",
    "    \"https://api.deepai.org/api/summarization\",\n",
    "    files={\n",
    "        'text': open('../data/experimental/interim.txt', 'rb'),\n",
    "    },\n",
    "    headers={'api-key': api_key}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language modeling, there is a concept called in-context learning Large.\n",
      "language models such\n",
      "model receives as input an optional description of that task along with some number of examples\n",
      "demonstrating the task, up until some final example that the model should complete itself.\n",
      "2.1 Language Modeling\n",
      "distribution of word sequences.\n",
      "you can find which is the ith missing word using a language model.\n",
      "2.2 Word Embedding\n",
      "Word embedding refers to the techniques of representing words in a corpus as n-dimensions\n",
      "It is because of word embedding that language models like RNNs, LSTMs,\n",
      "If two words have similar embedding then these words share some properties.\n",
      "are driven by the way embedding is constructed, for example, in word2vec two words with\n",
      "similar embedding are two words that often appear in the same context, which is not to say they\n",
      "The CBOW model learns the embedding by predicting\n",
      "The Skip-Gram model learns by\n",
      "predicting the surrounding words (context) given a current word.\n",
      "In-context Learning with Large Language Models - Degaga Wolde 2 / 5\n",
      "contextualized words embedding that capture word semantics in different contexts to address\n",
      "the issue of polysemous and the context-dependent nature of words.\n",
      "taking a sequence of word as input.\n",
      "Instead of using the complete sequence of inputs each time it predicts an output word, a\n",
      "Only a select few input words are given consideration.\n",
      "Text generation works by conditioning a model on some input text, transformed beforehand\n",
      "come first and which word come next, but transformer take all embedding at once.\n",
      "the transformer to also get the information about the position of a word in the input sequence.\n",
      "In-context learning is a mysterious emergent behavior in large language models (LMs) where\n",
      "In-context Learning with Large Language Models - Degaga Wolde 3 / 5\n",
      "In-context learning allows users to quickly build models for a new use case without worrying\n",
      "4 Large Language Modeling(LLMS)\n",
      "Trained on billions of words BERTs main advantage is that it utilizes bi-directional learning\n",
      "to gain context of words, meaning it understands the context of words by reading it both ways\n",
      "on the other hand learns context based on the words around it instead of just relying on the\n",
      "In-context Learning with Large Language Models - Degaga Wolde 4 / 5\n",
      "In-context Learning with Large Language Models - Degaga Wolde 5 / 5\n"
     ]
    }
   ],
   "source": [
    "print(r.json()['output'])\n",
    "with open('../data/output/experiment_output/summary.txt', 'w') as f:\n",
    "    f.write(r.json()['output'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example posting a local text file:\n",
    "\n",
    "r = requests.post(\n",
    "    \"https://api.deepai.org/api/text-tagging\",\n",
    "    files={\n",
    "        'text': open('../data/experimental/interim.txt', 'rb'),\n",
    "    },\n",
    "    headers={'api-key': api_key}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language modeling, there is a concept called in-context learning Large.\n",
      "language models such\n",
      "model receives as input an optional description of that task along with some number of examples\n",
      "demonstrating the task, up until some final example that the model should complete itself.\n",
      "2.1 Language Modeling\n",
      "distribution of word sequences.\n",
      "you can find which is the ith missing word using a language model.\n",
      "2.2 Word Embedding\n",
      "Word embedding refers to the techniques of representing words in a corpus as n-dimensions\n",
      "It is because of word embedding that language models like RNNs, LSTMs,\n",
      "If two words have similar embedding then these words share some properties.\n",
      "are driven by the way embedding is constructed, for example, in word2vec two words with\n",
      "similar embedding are two words that often appear in the same context, which is not to say they\n",
      "The CBOW model learns the embedding by predicting\n",
      "The Skip-Gram model learns by\n",
      "predicting the surrounding words (context) given a current word.\n",
      "In-context Learning with Large Language Models - Degaga Wolde 2 / 5\n",
      "contextualized words embedding that capture word semantics in different contexts to address\n",
      "the issue of polysemous and the context-dependent nature of words.\n",
      "taking a sequence of word as input.\n",
      "Instead of using the complete sequence of inputs each time it predicts an output word, a\n",
      "Only a select few input words are given consideration.\n",
      "Text generation works by conditioning a model on some input text, transformed beforehand\n",
      "come first and which word come next, but transformer take all embedding at once.\n",
      "the transformer to also get the information about the position of a word in the input sequence.\n",
      "In-context learning is a mysterious emergent behavior in large language models (LMs) where\n",
      "In-context Learning with Large Language Models - Degaga Wolde 3 / 5\n",
      "In-context learning allows users to quickly build models for a new use case without worrying\n",
      "4 Large Language Modeling(LLMS)\n",
      "Trained on billions of words BERTs main advantage is that it utilizes bi-directional learning\n",
      "to gain context of words, meaning it understands the context of words by reading it both ways\n",
      "on the other hand learns context based on the words around it instead of just relying on the\n",
      "In-context Learning with Large Language Models - Degaga Wolde 4 / 5\n",
      "In-context Learning with Large Language Models - Degaga Wolde 5 / 5\n"
     ]
    }
   ],
   "source": [
    "print(r.json()['output'])\n",
    "with open('../data/output/experiment_output/text_tagging.txt', 'w') as f:\n",
    "    f.write(r.json()['output'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('10A')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2be9f4616ca1b5198a62cd2082c7feed2682666a7b8fb219311f5849134be0bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
